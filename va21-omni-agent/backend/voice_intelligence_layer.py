"""
VA21 Voice Intelligence Layer - Comprehensive Voice Processing System

This module integrates multiple voice technologies for complete voice interaction:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VA21 Voice Intelligence Layer                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸŽ¤ Input Layer                                                             â”‚
â”‚  â”œâ”€â”€ Meta Omnilingual ASR (1,600+ languages) - Apache 2.0                  â”‚
â”‚  â”‚   â”œâ”€â”€ 7B model (high accuracy) - 14GB RAM                               â”‚
â”‚  â”‚   â”œâ”€â”€ 3B model (excellent) - 8GB RAM                                    â”‚
â”‚  â”‚   â”œâ”€â”€ 1B model (balanced) - 4GB RAM â† VA21 Default                      â”‚
â”‚  â”‚   â””â”€â”€ 300M model (low-power) - 2GB RAM                                  â”‚
â”‚  â”œâ”€â”€ Solus AI/Whisper (offline backup) - MIT                               â”‚
â”‚  â””â”€â”€ Zero-shot adaptation (custom dialects)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â° Wake Word Layer                                                          â”‚
â”‚  â””â”€â”€ Rhasspy (custom triggers) - MIT                                        â”‚
â”‚      â””â”€â”€ Custom wake words: "Hey VA21", "Om Vinayaka", etc.                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ›¡ï¸ Security Layer (Guardian AI Validation)                                 â”‚
â”‚  â”œâ”€â”€ Voice command security analysis                                        â”‚
â”‚  â”œâ”€â”€ Prompt injection detection                                             â”‚
â”‚  â”œâ”€â”€ User verification                                                      â”‚
â”‚  â””â”€â”€ LLM Guard integration - MIT                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ¤– Processing Layer                                                        â”‚
â”‚  â”œâ”€â”€ LangChain (AI orchestration) - MIT                                     â”‚
â”‚  â”œâ”€â”€ IBM Granite (reasoning) - Apache 2.0                                   â”‚
â”‚  â”œâ”€â”€ LMDeploy (efficient inference) - Apache 2.0                            â”‚
â”‚  â””â”€â”€ Command interpretation & context                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ—£ï¸ Output Layer                                                            â”‚
â”‚  â”œâ”€â”€ Piper TTS (fast synthesis) - MIT                                       â”‚
â”‚  â”œâ”€â”€ Kokoro TTS (premium quality) - Apache 2.0                              â”‚
â”‚  â””â”€â”€ Voice cloning (user's voice preservation)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Technology Stack:
| Component         | Technology              | License     | Purpose              |
|-------------------|-------------------------|-------------|----------------------|
| ASR (Primary)     | Meta Omnilingual ASR    | Apache 2.0  | 1,600+ languages     |
| ASR (Secondary)   | Whisper/Solus AI        | MIT         | Offline backup       |
| Wake Word         | Rhasspy                 | MIT         | Custom triggers      |
| TTS (Fast)        | Piper                   | MIT         | Fast synthesis       |
| TTS (Premium)     | Kokoro                  | Apache 2.0  | Premium voices       |
| LLM Processing    | LangChain + Granite     | MIT + Open  | AI reasoning         |
| Security          | Guardian AI + LLM Guard | Prop + MIT  | Safety layer         |

Model Selection Guide (RAM-based):
| Use Case          | Model Size | RAM Required | Accuracy   | Device           |
|-------------------|------------|--------------|------------|------------------|
| Low-Power Devices | 300M       | ~2GB         | Good       | Raspberry Pi     |
| Balanced          | 1B         | ~4GB         | Very Good  | Consumer laptops |
| High Performance  | 3B         | ~8GB         | Excellent  | Desktop/server   |
| Maximum Accuracy  | 7B         | ~14GB        | Best       | High-end hardware|

For VA21 (7GB RAM target):
- Use 1B model for voice input (4GB RAM)
- Leaves 3GB for other AI components
- Excellent accuracy for practical use

Om Vinayaka - Voice flows where language is no barrier.
"""

import os
import sys
import json
import threading
import time
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import deque


# =============================================================================
# ENUMS AND CONSTANTS
# =============================================================================

class VoiceComponent(Enum):
    """Voice system components."""
    ASR_PRIMARY = "asr_primary"       # Meta Omnilingual ASR
    ASR_SECONDARY = "asr_secondary"   # Whisper/Solus AI
    WAKE_WORD = "wake_word"           # Rhasspy
    TTS_FAST = "tts_fast"             # Piper
    TTS_PREMIUM = "tts_premium"       # Kokoro
    LLM_PROCESSING = "llm_processing" # LangChain + Granite
    SECURITY = "security"             # Guardian AI + LLM Guard


class ModelSize(Enum):
    """Model sizes for voice components."""
    TINY = "tiny"       # Minimal footprint
    SMALL = "small"     # 300M parameters
    MEDIUM = "medium"   # 1B parameters
    LARGE = "large"     # 3B parameters
    XLARGE = "xlarge"   # 7B parameters


class TTSQuality(Enum):
    """TTS quality levels."""
    FAST = "fast"         # Piper - low latency
    BALANCED = "balanced" # Piper high quality
    PREMIUM = "premium"   # Kokoro - best quality


class LanguageFamily(Enum):
    """Language families."""
    INDO_ARYAN = "indo_aryan"
    DRAVIDIAN = "dravidian"
    GERMANIC = "germanic"
    ROMANCE = "romance"
    SINO_TIBETAN = "sino_tibetan"
    OTHER = "other"


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ComponentInfo:
    """Information about a voice component."""
    name: str
    technology: str
    license: str
    github_url: str
    purpose: str
    ram_mb: int
    is_loaded: bool = False
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'technology': self.technology,
            'license': self.license,
            'github_url': self.github_url,
            'purpose': self.purpose,
            'ram_mb': self.ram_mb,
            'is_loaded': self.is_loaded,
        }


@dataclass
class ModelConfig:
    """Configuration for a model size."""
    size: ModelSize
    parameters: str
    ram_gb: float
    accuracy: str
    recommended_device: str
    
    def to_dict(self) -> Dict:
        return {
            'size': self.size.value,
            'parameters': self.parameters,
            'ram_gb': self.ram_gb,
            'accuracy': self.accuracy,
            'recommended_device': self.recommended_device,
        }


@dataclass
class VoiceCommand:
    """A processed voice command."""
    command_id: str
    audio_duration_seconds: float
    transcribed_text: str
    detected_language: str
    confidence: float
    processed_command: str
    response_text: str
    tts_quality: TTSQuality
    processing_time_ms: float
    security_approved: bool
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict:
        return {
            'command_id': self.command_id,
            'audio_duration_seconds': self.audio_duration_seconds,
            'transcribed_text': self.transcribed_text,
            'detected_language': self.detected_language,
            'confidence': self.confidence,
            'processed_command': self.processed_command,
            'response_text': self.response_text,
            'tts_quality': self.tts_quality.value,
            'processing_time_ms': self.processing_time_ms,
            'security_approved': self.security_approved,
            'timestamp': self.timestamp.isoformat(),
        }


@dataclass
class WakeWordConfig:
    """Wake word configuration."""
    phrase: str
    sensitivity: float
    enabled: bool
    
    def to_dict(self) -> Dict:
        return {
            'phrase': self.phrase,
            'sensitivity': self.sensitivity,
            'enabled': self.enabled,
        }


# =============================================================================
# COMPONENT REGISTRY
# =============================================================================

VOICE_COMPONENTS = {
    VoiceComponent.ASR_PRIMARY: ComponentInfo(
        name="Meta Omnilingual ASR",
        technology="Omnilingual wav2vec 2.0",
        license="Apache 2.0",
        github_url="https://github.com/facebookresearch/fairseq",
        purpose="1,600+ language speech recognition",
        ram_mb=4096,  # 1B model default
    ),
    VoiceComponent.ASR_SECONDARY: ComponentInfo(
        name="Whisper/Solus AI",
        technology="OpenAI Whisper",
        license="MIT",
        github_url="https://github.com/openai/whisper",
        purpose="Offline backup ASR",
        ram_mb=1500,
    ),
    VoiceComponent.WAKE_WORD: ComponentInfo(
        name="Rhasspy Wake Word",
        technology="Rhasspy/openWakeWord",
        license="MIT",
        github_url="https://github.com/rhasspy/rhasspy",
        purpose="Custom wake word detection",
        ram_mb=200,
    ),
    VoiceComponent.TTS_FAST: ComponentInfo(
        name="Piper TTS",
        technology="Piper Neural TTS",
        license="MIT",
        github_url="https://github.com/rhasspy/piper",
        purpose="Fast text-to-speech synthesis",
        ram_mb=500,
    ),
    VoiceComponent.TTS_PREMIUM: ComponentInfo(
        name="Kokoro TTS",
        technology="Kokoro-82M Neural TTS",
        license="Apache 2.0",
        github_url="https://github.com/remsky/Kokoro-FastAPI",
        purpose="Premium quality voices",
        ram_mb=800,
    ),
    VoiceComponent.LLM_PROCESSING: ComponentInfo(
        name="LangChain + Granite",
        technology="LangChain + IBM Granite",
        license="MIT + Apache 2.0",
        github_url="https://github.com/langchain-ai/langchain",
        purpose="AI reasoning and orchestration",
        ram_mb=2048,
    ),
    VoiceComponent.SECURITY: ComponentInfo(
        name="Guardian AI + LLM Guard",
        technology="VA21 Guardian + ProtectAI LLM Guard",
        license="Proprietary + MIT",
        github_url="https://github.com/protectai/llm-guard",
        purpose="Voice command security",
        ram_mb=512,
    ),
}

# Model size configurations
MODEL_CONFIGS = {
    ModelSize.TINY: ModelConfig(
        size=ModelSize.TINY,
        parameters="50M",
        ram_gb=0.5,
        accuracy="Basic",
        recommended_device="Embedded systems",
    ),
    ModelSize.SMALL: ModelConfig(
        size=ModelSize.SMALL,
        parameters="300M",
        ram_gb=2.0,
        accuracy="Good",
        recommended_device="Raspberry Pi",
    ),
    ModelSize.MEDIUM: ModelConfig(
        size=ModelSize.MEDIUM,
        parameters="1B",
        ram_gb=4.0,
        accuracy="Very Good",
        recommended_device="Consumer laptops",
    ),
    ModelSize.LARGE: ModelConfig(
        size=ModelSize.LARGE,
        parameters="3B",
        ram_gb=8.0,
        accuracy="Excellent",
        recommended_device="Desktop/server",
    ),
    ModelSize.XLARGE: ModelConfig(
        size=ModelSize.XLARGE,
        parameters="7B",
        ram_gb=14.0,
        accuracy="Best",
        recommended_device="High-end hardware",
    ),
}


# =============================================================================
# MAIN VOICE INTELLIGENCE LAYER
# =============================================================================

class VoiceIntelligenceLayer:
    """
    VA21 Voice Intelligence Layer
    
    Comprehensive voice processing system integrating:
    - Meta Omnilingual ASR (1,600+ languages) - Apache 2.0
    - Whisper/Solus AI (offline backup) - MIT
    - Rhasspy (wake word detection) - MIT
    - Piper TTS (fast synthesis) - MIT
    - Kokoro TTS (premium quality) - Apache 2.0
    - LangChain + Granite (AI processing) - MIT + Apache 2.0
    - Guardian AI + LLM Guard (security) - Proprietary + MIT
    
    Model Selection (RAM-based):
    - 300M: 2GB RAM (Raspberry Pi)
    - 1B: 4GB RAM (Consumer laptops) â† VA21 Default
    - 3B: 8GB RAM (Desktop/server)
    - 7B: 14GB RAM (High-end hardware)
    
    For VA21 (7GB RAM target):
    - Use 1B ASR model (4GB RAM)
    - Leaves 3GB for other AI components
    """
    
    VERSION = "1.0.0"
    
    # Default wake words
    DEFAULT_WAKE_WORDS = [
        WakeWordConfig("Hey VA21", 0.7, True),
        WakeWordConfig("Om Vinayaka", 0.6, True),
        WakeWordConfig("Computer", 0.8, False),
    ]
    
    # Supported Indian languages
    INDIAN_LANGUAGES = {
        'hi': ('Hindi', 'à¤¹à¤¿à¤¨à¥à¤¦à¥€', LanguageFamily.INDO_ARYAN),
        'ta': ('Tamil', 'à®¤à®®à®¿à®´à¯', LanguageFamily.DRAVIDIAN),
        'te': ('Telugu', 'à°¤à±†à°²à±à°—à±', LanguageFamily.DRAVIDIAN),
        'kn': ('Kannada', 'à²•à²¨à³à²¨à²¡', LanguageFamily.DRAVIDIAN),
        'ml': ('Malayalam', 'à´®à´²à´¯à´¾à´³à´‚', LanguageFamily.DRAVIDIAN),
        'bn': ('Bengali', 'à¦¬à¦¾à¦‚à¦²à¦¾', LanguageFamily.INDO_ARYAN),
        'mr': ('Marathi', 'à¤®à¤°à¤¾à¤ à¥€', LanguageFamily.INDO_ARYAN),
        'gu': ('Gujarati', 'àª—à«àªœàª°àª¾àª¤à«€', LanguageFamily.INDO_ARYAN),
        'pa': ('Punjabi', 'à¨ªà©°à¨œà¨¾à¨¬à©€', LanguageFamily.INDO_ARYAN),
        'or': ('Odia', 'à¬“à¬¡à¬¼à¬¿à¬†', LanguageFamily.INDO_ARYAN),
        'as': ('Assamese', 'à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾', LanguageFamily.INDO_ARYAN),
        'ur': ('Urdu', 'Ø§Ø±Ø¯Ùˆ', LanguageFamily.INDO_ARYAN),
        'awa': ('Awadhi', 'à¤…à¤µà¤§à¥€', LanguageFamily.INDO_ARYAN),
        'mai': ('Maithili', 'à¤®à¥ˆà¤¥à¤¿à¤²à¥€', LanguageFamily.INDO_ARYAN),
        'hne': ('Chhattisgarhi', 'à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼à¥€', LanguageFamily.INDO_ARYAN),
        'tcy': ('Tulu', 'à²¤à³à²³à³', LanguageFamily.DRAVIDIAN),
        'bho': ('Bhojpuri', 'à¤­à¥‹à¤œà¤ªà¥à¤°à¥€', LanguageFamily.INDO_ARYAN),
    }
    
    def __init__(
        self,
        model_size: ModelSize = ModelSize.MEDIUM,  # 1B model (4GB) - VA21 default
        tts_quality: TTSQuality = TTSQuality.BALANCED,
        enable_wake_word: bool = True,
        enable_security: bool = True,
        guardian=None,
        data_dir: str = "data/voice"
    ):
        """
        Initialize the Voice Intelligence Layer.
        
        Args:
            model_size: ASR model size (default: MEDIUM/1B for 7GB RAM systems)
            tts_quality: TTS quality level
            enable_wake_word: Enable wake word detection
            enable_security: Enable Guardian AI security
            guardian: VA21 Guardian AI instance
            data_dir: Directory for voice data
        """
        self.model_size = model_size
        self.tts_quality = tts_quality
        self.enable_wake_word = enable_wake_word
        self.enable_security = enable_security
        self.guardian = guardian
        self.data_dir = data_dir
        
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(os.path.join(data_dir, "audio"), exist_ok=True)
        os.makedirs(os.path.join(data_dir, "transcripts"), exist_ok=True)
        
        # Component states
        self.components = dict(VOICE_COMPONENTS)
        self.loaded_components: set = set()
        
        # Wake words
        self.wake_words = list(self.DEFAULT_WAKE_WORDS)
        
        # Lock for thread safety
        self._lock = threading.RLock()
        
        # Metrics
        self.metrics = {
            'commands_processed': 0,
            'wake_words_detected': 0,
            'security_blocks': 0,
            'languages_used': set(),
            'total_audio_seconds': 0,
            'tts_generations': 0,
        }
        
        # Calculate RAM usage
        self._calculate_ram_usage()
        
        print(f"[VoiceIntelligenceLayer] Initialized v{self.VERSION}")
        print(f"[VoiceIntelligenceLayer] ASR Model: {model_size.value} ({MODEL_CONFIGS[model_size].parameters})")
        print(f"[VoiceIntelligenceLayer] TTS Quality: {tts_quality.value}")
        print(f"[VoiceIntelligenceLayer] Estimated RAM: {self.estimated_ram_gb:.1f}GB")
    
    def _calculate_ram_usage(self):
        """Calculate estimated RAM usage based on configuration."""
        asr_ram = MODEL_CONFIGS[self.model_size].ram_gb * 1024  # Convert to MB
        
        tts_ram = 500 if self.tts_quality == TTSQuality.FAST else 800
        
        other_ram = sum([
            200 if self.enable_wake_word else 0,  # Wake word
            512 if self.enable_security else 0,    # Security
            256,  # Base overhead
        ])
        
        self.estimated_ram_mb = asr_ram + tts_ram + other_ram
        self.estimated_ram_gb = self.estimated_ram_mb / 1024
    
    # =========================================================================
    # MODEL SELECTION
    # =========================================================================
    
    def get_recommended_model(self, available_ram_gb: float) -> ModelSize:
        """
        Get recommended model size based on available RAM.
        
        VA21 Recommendation (7GB target):
        - Use 1B model (4GB) for voice
        - Leaves 3GB for other AI components
        
        Args:
            available_ram_gb: Available system RAM in GB
            
        Returns:
            Recommended ModelSize
        """
        # Reserve 3GB for other components
        voice_ram_budget = available_ram_gb - 3.0
        
        for size in reversed(list(ModelSize)):
            config = MODEL_CONFIGS[size]
            if config.ram_gb <= voice_ram_budget:
                return size
        
        return ModelSize.SMALL
    
    def get_model_configs(self) -> List[Dict]:
        """Get all model configurations."""
        return [config.to_dict() for config in MODEL_CONFIGS.values()]
    
    # =========================================================================
    # VOICE PROCESSING PIPELINE
    # =========================================================================
    
    def process_voice_command(
        self,
        audio_path: str = None,
        audio_data: bytes = None,
        language: str = None
    ) -> VoiceCommand:
        """
        Process a voice command through the full pipeline.
        
        Pipeline:
        1. Wake word detection (if enabled)
        2. Speech-to-text (Meta Omnilingual ASR)
        3. Security validation (Guardian AI + LLM Guard)
        4. LLM processing (LangChain + Granite)
        5. Text-to-speech response (Piper/Kokoro)
        
        Args:
            audio_path: Path to audio file
            audio_data: Raw audio data
            language: Target language code
            
        Returns:
            VoiceCommand with results
        """
        start_time = time.time()
        command_id = hashlib.md5(f"{time.time()}".encode()).hexdigest()[:12]
        
        with self._lock:
            self.metrics['commands_processed'] += 1
        
        # Step 1: Transcribe speech (Meta Omnilingual ASR)
        transcribed_text, detected_language, confidence = self._transcribe_audio(
            audio_path, audio_data, language
        )
        
        # Step 2: Security validation
        security_approved = True
        if self.enable_security:
            security_approved = self._validate_security(transcribed_text)
            if not security_approved:
                self.metrics['security_blocks'] += 1
        
        # Step 3: Process command (LangChain + Granite)
        if security_approved:
            processed_command = self._process_command(transcribed_text, detected_language)
            response_text = self._generate_response(processed_command, detected_language)
        else:
            processed_command = "[BLOCKED]"
            response_text = "I'm sorry, that command was blocked for security reasons."
        
        # Step 4: Generate TTS response
        self._generate_tts(response_text, detected_language)
        
        processing_time = (time.time() - start_time) * 1000
        
        with self._lock:
            self.metrics['languages_used'].add(detected_language)
        
        return VoiceCommand(
            command_id=command_id,
            audio_duration_seconds=5.0,  # Mock duration
            transcribed_text=transcribed_text,
            detected_language=detected_language,
            confidence=confidence,
            processed_command=processed_command,
            response_text=response_text,
            tts_quality=self.tts_quality,
            processing_time_ms=processing_time,
            security_approved=security_approved,
        )
    
    def _transcribe_audio(
        self,
        audio_path: str = None,
        audio_data: bytes = None,
        language: str = None
    ) -> Tuple[str, str, float]:
        """Transcribe audio using Meta Omnilingual ASR."""
        # In production, use actual ASR model
        # For now, return mock transcription
        
        detected_language = language or 'hi'
        
        mock_transcriptions = {
            'hi': 'à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤•à¥ƒà¤ªà¤¯à¤¾ à¤¬à¥ˆà¤•à¤…à¤ª à¤²à¥‡à¤‚',
            'ta': 'à®µà®£à®•à¯à®•à®®à¯, à®¤à®¯à®µà¯à®šà¯†à®¯à¯à®¤à¯ à®•à®¾à®ªà¯à®ªà¯à®ªà¯à®ªà®¿à®°à®¤à®¿ à®Žà®Ÿà¯à®•à¯à®•à®µà¯à®®à¯',
            'te': 'à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¦à°¯à°šà±‡à°¸à°¿ à°¬à±à°¯à°¾à°•à°ªà± à°¤à±€à°¸à±à°•à±‹à°‚à°¡à°¿',
            'en': 'Hello, please take a backup',
        }
        
        text = mock_transcriptions.get(detected_language, f'[Transcription in {detected_language}]')
        confidence = 0.95 if detected_language in self.INDIAN_LANGUAGES else 0.90
        
        return text, detected_language, confidence
    
    def _validate_security(self, text: str) -> bool:
        """Validate command security using Guardian AI + LLM Guard."""
        # Check for dangerous patterns
        dangerous_patterns = [
            'delete all', 'format', 'rm -rf', 'sudo rm',
            'password', 'credential', 'private key',
        ]
        
        text_lower = text.lower()
        for pattern in dangerous_patterns:
            if pattern in text_lower:
                return False
        
        # Additional Guardian AI validation
        if self.guardian:
            try:
                result = self.guardian.analyze(text)
                return result.get('safe', True)
            except Exception:
                pass
        
        return True
    
    def _process_command(self, text: str, language: str) -> str:
        """Process command using LangChain + Granite."""
        # In production, use actual LLM processing
        # For now, return normalized command
        
        # Simple command extraction
        command_mapping = {
            'backup': 'BACKUP_COMMAND',
            'à¤¬à¥ˆà¤•à¤…à¤ª': 'BACKUP_COMMAND',
            'restore': 'RESTORE_COMMAND',
            'help': 'HELP_COMMAND',
            'à¤®à¤¦à¤¦': 'HELP_COMMAND',
        }
        
        text_lower = text.lower()
        for trigger, command in command_mapping.items():
            if trigger in text_lower:
                return command
        
        return 'GENERAL_QUERY'
    
    def _generate_response(self, command: str, language: str) -> str:
        """Generate response text."""
        responses = {
            'BACKUP_COMMAND': {
                'hi': 'à¤¬à¥ˆà¤•à¤…à¤ª à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤¶à¥à¤°à¥‚ à¤¹à¥‹ à¤—à¤ˆ à¤¹à¥ˆà¥¤',
                'ta': 'à®•à®¾à®ªà¯à®ªà¯à®ªà¯à®ªà®¿à®°à®¤à®¿ à®šà¯†à®¯à®²à¯à®®à¯à®±à¯ˆ à®¤à¯Šà®Ÿà®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿà®¤à¯.',
                'te': 'à°¬à±à°¯à°¾à°•à°ªà± à°ªà±à°°à°•à±à°°à°¿à°¯ à°ªà±à°°à°¾à°°à°‚à°­à°®à±ˆà°‚à°¦à°¿.',
                'en': 'Backup process has been initiated.',
            },
            'RESTORE_COMMAND': {
                'hi': 'à¤ªà¥à¤¨à¤°à¥à¤¸à¥à¤¥à¤¾à¤ªà¤¨à¤¾ à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤¶à¥à¤°à¥‚ à¤¹à¥‹ à¤—à¤ˆ à¤¹à¥ˆà¥¤',
                'en': 'Restore process has been initiated.',
            },
            'HELP_COMMAND': {
                'hi': 'à¤®à¥ˆà¤‚ VA21 OS à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥‚à¤‚à¥¤ à¤®à¥ˆà¤‚ à¤†à¤ªà¤•à¥€ à¤•à¥ˆà¤¸à¥‡ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤‚?',
                'en': 'I am the VA21 OS assistant. How can I help you?',
            },
        }
        
        command_responses = responses.get(command, {'en': 'I understand. Processing your request.'})
        return command_responses.get(language, command_responses.get('en', 'Processing...'))
    
    def _generate_tts(self, text: str, language: str):
        """Generate TTS audio using Piper or Kokoro."""
        # In production, use actual TTS engines
        # Piper for fast synthesis, Kokoro for premium quality
        
        with self._lock:
            self.metrics['tts_generations'] += 1
        
        # Mock TTS generation
        if self.tts_quality == TTSQuality.FAST:
            # Use Piper
            pass
        else:
            # Use Kokoro
            pass
    
    # =========================================================================
    # WAKE WORD
    # =========================================================================
    
    def detect_wake_word(self, audio_data: bytes) -> Optional[str]:
        """
        Detect wake word in audio using Rhasspy.
        
        Default wake words:
        - "Hey VA21"
        - "Om Vinayaka"
        - "Computer" (disabled by default)
        """
        if not self.enable_wake_word:
            return None
        
        # In production, use Rhasspy/openWakeWord
        # For now, return mock detection
        
        for wake_word in self.wake_words:
            if wake_word.enabled:
                # Simulate detection with probability
                if hash(audio_data) % 10 < wake_word.sensitivity * 10:
                    self.metrics['wake_words_detected'] += 1
                    return wake_word.phrase
        
        return None
    
    def add_wake_word(self, phrase: str, sensitivity: float = 0.7) -> bool:
        """Add a custom wake word."""
        with self._lock:
            self.wake_words.append(WakeWordConfig(phrase, sensitivity, True))
        return True
    
    # =========================================================================
    # MULTILINGUAL SUPPORT
    # =========================================================================
    
    def get_supported_languages(self) -> Dict[str, Tuple[str, str, str]]:
        """Get all supported Indian languages."""
        return {
            code: (name, native, family.value)
            for code, (name, native, family) in self.INDIAN_LANGUAGES.items()
        }
    
    def translate_for_tts(self, text: str, from_lang: str, to_lang: str) -> str:
        """
        Translate text for TTS output.
        
        Example:
        User speaks in Telugu: "à°¬à±à°¯à°¾à°•à°ªà± à°¤à±€à°¸à±à°•à±‹à°‚à°¡à°¿" (Take backup)
            â†“
        Meta Omnilingual ASR â†’ English: "Take backup"
            â†“
        Guardian AI validates command
            â†“
        Orchestrator AI executes backup
            â†“
        Response in Telugu via TTS
        """
        # In production, use translation model
        # For now, return original text
        return text
    
    # =========================================================================
    # STATUS AND METRICS
    # =========================================================================
    
    def get_status(self) -> Dict:
        """Get system status."""
        return {
            'version': self.VERSION,
            'model_size': self.model_size.value,
            'model_config': MODEL_CONFIGS[self.model_size].to_dict(),
            'tts_quality': self.tts_quality.value,
            'wake_word_enabled': self.enable_wake_word,
            'security_enabled': self.enable_security,
            'estimated_ram_gb': self.estimated_ram_gb,
            'components': {
                comp.value: info.to_dict() 
                for comp, info in self.components.items()
            },
            'wake_words': [ww.to_dict() for ww in self.wake_words if ww.enabled],
            'indian_languages_count': len(self.INDIAN_LANGUAGES),
            'metrics': {
                'commands_processed': self.metrics['commands_processed'],
                'wake_words_detected': self.metrics['wake_words_detected'],
                'security_blocks': self.metrics['security_blocks'],
                'languages_used': list(self.metrics['languages_used']),
                'tts_generations': self.metrics['tts_generations'],
            },
        }
    
    def get_acknowledgment(self) -> str:
        """Get acknowledgment text for all integrated technologies."""
        return """
ðŸŽ¤ **VA21 Voice Intelligence Layer - Acknowledgments**

VA21 OS gratefully acknowledges the following open source projects:

### Speech Recognition (ASR)

**Meta Omnilingual ASR** â­â­â­â­â­
License: Apache 2.0
Released: November 2025
- 1,600+ languages (including 500+ low-resource)
- Indian languages: Hindi, Tamil, Telugu, Kannada, Bengali, Marathi, and 100+ more
- Zero-shot learning: Extend to 5,400+ languages
- Model sizes: 300M, 1B, 3B, 7B parameters
Thank you, Meta FAIR team! ðŸ™

**OpenAI Whisper / Solus AI**
License: MIT
https://github.com/openai/whisper
- Offline backup ASR
- Excellent accuracy for common languages
Thank you, OpenAI! ðŸ™

### Wake Word Detection

**Rhasspy**
License: MIT
https://github.com/rhasspy/rhasspy
- Custom wake word training
- Privacy-focused (fully offline)
- Custom triggers: "Hey VA21", "Om Vinayaka"
Thank you, Rhasspy team! ðŸ™

### Text-to-Speech (TTS)

**Piper TTS**
License: MIT
https://github.com/rhasspy/piper
- Fast neural TTS synthesis
- Multiple languages and voices
- Low latency for real-time response
Thank you, Rhasspy/Piper team! ðŸ™

**Kokoro TTS**
License: Apache 2.0
https://github.com/remsky/Kokoro-FastAPI
- Premium quality neural voices
- 82M parameter model
- Expressive and natural speech
Thank you, Kokoro team! ðŸ™

### LLM Processing

**LangChain**
License: MIT
https://github.com/langchain-ai/langchain
- AI orchestration framework
- Chain-of-thought processing
Thank you, LangChain team! ðŸ™

**IBM Granite**
License: Apache 2.0
https://github.com/ibm-granite
- Enterprise-grade LLM
- Reasoning and understanding
Thank you, IBM Research! ðŸ™

### Security

**LLM Guard (ProtectAI)**
License: MIT
https://github.com/protectai/llm-guard
- Prompt injection detection
- Output validation
- Toxicity filtering
Thank you, ProtectAI! ðŸ™

**LMDeploy**
License: Apache 2.0
https://github.com/InternLM/lmdeploy
- Efficient LLM deployment
- Quantization support
Thank you, InternLM team! ðŸ™

### Model Selection Guide

| Use Case          | Model Size | RAM Required | Accuracy   |
|-------------------|------------|--------------|------------|
| Low-Power Devices | 300M       | ~2GB         | Good       |
| Balanced          | 1B         | ~4GB         | Very Good  |
| High Performance  | 3B         | ~8GB         | Excellent  |
| Maximum Accuracy  | 7B         | ~14GB        | Best       |

**VA21 Default (7GB RAM target):**
- 1B ASR model (4GB RAM)
- Leaves 3GB for other AI components
- Excellent accuracy for practical use

**Multilingual Voice Flow:**
```
User speaks in Telugu: "à°¬à±à°¯à°¾à°•à°ªà± à°¤à±€à°¸à±à°•à±‹à°‚à°¡à°¿" (Take backup)
    â†“
Meta Omnilingual ASR â†’ English: "Take backup"
    â†“
Guardian AI validates command
    â†“
Orchestrator AI executes backup
    â†“
Response in Telugu via TTS
```

Om Vinayaka - Voice flows where language is no barrier. ðŸ™
"""


# =============================================================================
# SINGLETON
# =============================================================================

_voice_layer: Optional[VoiceIntelligenceLayer] = None


def get_voice_intelligence_layer(
    model_size: ModelSize = ModelSize.MEDIUM,
    tts_quality: TTSQuality = TTSQuality.BALANCED,
    guardian=None
) -> VoiceIntelligenceLayer:
    """Get the Voice Intelligence Layer singleton instance."""
    global _voice_layer
    if _voice_layer is None:
        _voice_layer = VoiceIntelligenceLayer(
            model_size=model_size,
            tts_quality=tts_quality,
            guardian=guardian
        )
    return _voice_layer


if __name__ == "__main__":
    # Test the Voice Intelligence Layer
    print("\n=== VA21 Voice Intelligence Layer Test ===")
    
    layer = get_voice_intelligence_layer()
    
    print("\n--- Status ---")
    print(json.dumps(layer.get_status(), indent=2, default=str))
    
    print("\n--- Model Configurations ---")
    for config in layer.get_model_configs():
        print(f"  {config['size']}: {config['parameters']} ({config['ram_gb']}GB) - {config['accuracy']}")
    
    print("\n--- Model Recommendations ---")
    for ram in [4, 7, 8, 16]:
        recommended = layer.get_recommended_model(ram)
        print(f"  {ram}GB RAM â†’ {recommended.value} model")
    
    print("\n--- Supported Indian Languages ---")
    languages = layer.get_supported_languages()
    for code, (name, native, family) in list(languages.items())[:5]:
        print(f"  {code}: {name} ({native})")
    print(f"  ... and {len(languages) - 5} more")
    
    print("\n--- Testing Voice Command ---")
    result = layer.process_voice_command(language='hi')
    print(f"  Transcribed: {result.transcribed_text}")
    print(f"  Command: {result.processed_command}")
    print(f"  Response: {result.response_text}")
    print(f"  Security: {'âœ“ Approved' if result.security_approved else 'âœ— Blocked'}")
    
    print("\n--- Acknowledgment ---")
    print(layer.get_acknowledgment())
